{"id": "2506.04515", "pdf": "https://arxiv.org/pdf/2506.04515", "abs": "https://arxiv.org/abs/2506.04515", "authors": ["Salil Patel"], "title": "The Latent Space Hypothesis: Toward Universal Medical Representation Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "51 pages, 12 figures. A position paper examining the latent space\n  hypothesis - the proposition that diverse medical data can be represented in\n  shared latent spaces reflecting fundamental biological processes. The paper\n  discusses theoretical foundations, reviews supporting evidence, and considers\n  potential implications for medical AI and representation learning", "summary": "Medical data range from genomic sequences and retinal photographs to\nstructured laboratory results and unstructured clinical narratives. Although\nthese modalities appear disparate, many encode convergent information about a\nsingle underlying physiological state. The Latent Space Hypothesis frames each\nobservation as a projection of a unified, hierarchically organized manifold --\nmuch like shadows cast by the same three-dimensional object. Within this\nlearned geometric representation, an individual's health status occupies a\npoint, disease progression traces a trajectory, and therapeutic intervention\ncorresponds to a directed vector. Interpreting heterogeneous evidence in a\nshared space provides a principled way to re-examine eponymous conditions --\nsuch as Parkinson's or Crohn's -- that often mask multiple pathophysiological\nentities and involve broader anatomical domains than once believed. By\nrevealing sub-trajectories and patient-specific directions of change, the\nframework supplies a quantitative rationale for personalised diagnosis,\nlongitudinal monitoring, and tailored treatment, moving clinical practice away\nfrom grouping by potentially misleading labels toward navigation of each\nperson's unique trajectory. Challenges remain -- bias amplification, data\nscarcity for rare disorders, privacy, and the correlation-causation divide --\nbut scale-aware encoders, continual learning on longitudinal data streams, and\nperturbation-based validation offer plausible paths forward.", "AI": {"tldr": "The paper proposes a shared latent space for heterogeneous medical data to enable personalized medicine, but challenges like bias, data scarcity, and privacy remain. ", "motivation": "Eponymous conditions often mask multiple pathophysiological entities and involve broader anatomical domains than once believed.", "method": "The Latent Space Hypothesis frames each observation as a projection of a unified, hierarchically organized manifold.", "result": "The framework supplies a quantitative rationale for personalised diagnosis, longitudinal monitoring, and tailored treatment.", "conclusion": "A shared latent space can be used to represent heterogeneous medical data, enabling personalized diagnosis and treatment."}}
{"id": "2506.04235", "pdf": "https://arxiv.org/pdf/2506.04235", "abs": "https://arxiv.org/abs/2506.04235", "authors": ["Xinyan Zhao", "Yi-Ching Tang", "Akshita Singh", "Victor J Cantu", "KwanHo An", "Junseok Lee", "Adam E Stogsdill", "Ashwin Kumar Ramesh", "Zhiqiang An", "Xiaoqian Jiang", "Yejin Kim"], "title": "Benchmark for Antibody Binding Affinity Maturation and Design", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG", "q-bio.BM"], "comment": null, "summary": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking\nframework for antibody binding affinity maturation and design. Unlike existing\nantibody evaluation strategies that rely on antibody alone and its similarity\nto natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench\nconsiders an antibody-antigen (Ab-Ag) complex as a functional unit and\nevaluates the potential of an antibody design binding to given antigen by\nmeasuring protein model's likelihood on the Ab-Ag complex. We first curate,\nstandardize, and share 9 datasets containing 9 antigens (involving influenza,\nanti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain\nmutated antibodies. Using these datasets, we systematically compare 14 protein\nmodels including masked language models, autoregressive language models,\ninverse folding models, diffusion-based generative models, and geometric graph\nmodels. The correlation between model likelihood and experimental affinity\nvalues is used to evaluate model performance. Additionally, in a case study to\nincrease binding affinity of antibody F045-092 to antigen influenza H1N1, we\nevaluate the generative power of the top-performing models by sampling a set of\nnew antibodies binding to the antigen and ranking them based on structural\nintegrity and biophysical properties of the Ab-Ag complex. As a result,\nstructure-conditioned inverse folding models outperform others in both affinity\ncorrelation and generation tasks. Overall, AbBiBench provides a unified,\nbiologically grounded evaluation framework to facilitate the development of\nmore effective, function-aware antibody design models.", "AI": {"tldr": "AbBiBench is a new benchmark for antibody binding affinity maturation and design that considers the antibody-antigen complex as a functional unit. Structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks. ", "motivation": "Unlike existing antibody evaluation strategies that rely on antibody alone and its similarity to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench considers an antibody-antigen (Ab-Ag) complex as a functional unit and evaluates the potential of an antibody design binding to given antigen by measuring protein model's likelihood on the Ab-Ag complex.", "method": "We first curate, standardize, and share 9 datasets containing 9 antigens (involving influenza, anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain mutated antibodies. Using these datasets, we systematically compare 14 protein models including masked language models, autoregressive language models, inverse folding models, diffusion-based generative models, and geometric graph models.", "result": "structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks.", "conclusion": "AbBiBench provides a unified, biologically grounded evaluation framework to facilitate the development of more effective, function-aware antibody design models."}}
{"id": "2506.04515", "pdf": "https://arxiv.org/pdf/2506.04515", "abs": "https://arxiv.org/abs/2506.04515", "authors": ["Salil Patel"], "title": "The Latent Space Hypothesis: Toward Universal Medical Representation Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "51 pages, 12 figures. A position paper examining the latent space\n  hypothesis - the proposition that diverse medical data can be represented in\n  shared latent spaces reflecting fundamental biological processes. The paper\n  discusses theoretical foundations, reviews supporting evidence, and considers\n  potential implications for medical AI and representation learning", "summary": "Medical data range from genomic sequences and retinal photographs to\nstructured laboratory results and unstructured clinical narratives. Although\nthese modalities appear disparate, many encode convergent information about a\nsingle underlying physiological state. The Latent Space Hypothesis frames each\nobservation as a projection of a unified, hierarchically organized manifold --\nmuch like shadows cast by the same three-dimensional object. Within this\nlearned geometric representation, an individual's health status occupies a\npoint, disease progression traces a trajectory, and therapeutic intervention\ncorresponds to a directed vector. Interpreting heterogeneous evidence in a\nshared space provides a principled way to re-examine eponymous conditions --\nsuch as Parkinson's or Crohn's -- that often mask multiple pathophysiological\nentities and involve broader anatomical domains than once believed. By\nrevealing sub-trajectories and patient-specific directions of change, the\nframework supplies a quantitative rationale for personalised diagnosis,\nlongitudinal monitoring, and tailored treatment, moving clinical practice away\nfrom grouping by potentially misleading labels toward navigation of each\nperson's unique trajectory. Challenges remain -- bias amplification, data\nscarcity for rare disorders, privacy, and the correlation-causation divide --\nbut scale-aware encoders, continual learning on longitudinal data streams, and\nperturbation-based validation offer plausible paths forward.", "AI": {"tldr": "The Latent Space Hypothesis is introduced to interpret heterogeneous medical evidence in a shared space, providing a quantitative rationale for personalised diagnosis, longitudinal monitoring, and tailored treatment. Challenges remain, but plausible paths forward are offered.", "motivation": "Interpreting heterogeneous evidence in a shared space provides a principled way to re-examine eponymous conditions -- such as Parkinson's or Crohn's -- that often mask multiple pathophysiological entities and involve broader anatomical domains than once believed.", "method": "The Latent Space Hypothesis frames each observation as a projection of a unified, hierarchically organized manifold.", "result": "Within this learned geometric representation, an individual's health status occupies a point, disease progression traces a trajectory, and therapeutic intervention corresponds to a directed vector.", "conclusion": "This framework supplies a quantitative rationale for personalised diagnosis, longitudinal monitoring, and tailored treatment, moving clinical practice away from grouping by potentially misleading labels toward navigation of each person's unique trajectory."}}
{"id": "2506.04235", "pdf": "https://arxiv.org/pdf/2506.04235", "abs": "https://arxiv.org/abs/2506.04235", "authors": ["Xinyan Zhao", "Yi-Ching Tang", "Akshita Singh", "Victor J Cantu", "KwanHo An", "Junseok Lee", "Adam E Stogsdill", "Ashwin Kumar Ramesh", "Zhiqiang An", "Xiaoqian Jiang", "Yejin Kim"], "title": "Benchmark for Antibody Binding Affinity Maturation and Design", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG", "q-bio.BM"], "comment": null, "summary": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking\nframework for antibody binding affinity maturation and design. Unlike existing\nantibody evaluation strategies that rely on antibody alone and its similarity\nto natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench\nconsiders an antibody-antigen (Ab-Ag) complex as a functional unit and\nevaluates the potential of an antibody design binding to given antigen by\nmeasuring protein model's likelihood on the Ab-Ag complex. We first curate,\nstandardize, and share 9 datasets containing 9 antigens (involving influenza,\nanti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain\nmutated antibodies. Using these datasets, we systematically compare 14 protein\nmodels including masked language models, autoregressive language models,\ninverse folding models, diffusion-based generative models, and geometric graph\nmodels. The correlation between model likelihood and experimental affinity\nvalues is used to evaluate model performance. Additionally, in a case study to\nincrease binding affinity of antibody F045-092 to antigen influenza H1N1, we\nevaluate the generative power of the top-performing models by sampling a set of\nnew antibodies binding to the antigen and ranking them based on structural\nintegrity and biophysical properties of the Ab-Ag complex. As a result,\nstructure-conditioned inverse folding models outperform others in both affinity\ncorrelation and generation tasks. Overall, AbBiBench provides a unified,\nbiologically grounded evaluation framework to facilitate the development of\nmore effective, function-aware antibody design models.", "AI": {"tldr": "AbBiBench\u662f\u4e00\u4e2a\u6297\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u6210\u719f\u548c\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b83\u8003\u8651\u4e86\u6297\u4f53-\u6297\u539f\u590d\u5408\u7269\uff0c\u5e76\u4f7f\u7528\u6a21\u578b\u53ef\u80fd\u6027\u6765\u8bc4\u4f30\u7ed3\u5408\u4eb2\u548c\u529b\u3002\u7ed3\u6784\u6761\u4ef6\u9006\u6298\u53e0\u6a21\u578b\u8868\u73b0\u6700\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u6297\u4f53\u8bc4\u4f30\u7b56\u7565\u4f9d\u8d56\u4e8e\u6297\u4f53\u672c\u8eab\u53ca\u5176\u4e0e\u5929\u7136\u6297\u4f53\u7684\u76f8\u4f3c\u6027\uff08\u4f8b\u5982\uff0c\u6c28\u57fa\u9178\u540c\u4e00\u6027\u7387\u3001\u7ed3\u6784RMSD\uff09\uff0c\u800cAbBiBench\u5c06\u6297\u4f53-\u6297\u539f\uff08Ab-Ag\uff09\u590d\u5408\u7269\u89c6\u4e3a\u4e00\u4e2a\u529f\u80fd\u5355\u5143\uff0c\u5e76\u901a\u8fc7\u6d4b\u91cf\u86cb\u767d\u8d28\u6a21\u578b\u5728Ab-Ag\u590d\u5408\u7269\u4e0a\u7684\u53ef\u80fd\u6027\u6765\u8bc4\u4f30\u6297\u4f53\u8bbe\u8ba1\u4e0e\u7ed9\u5b9a\u6297\u539f\u7ed3\u5408\u7684\u6f5c\u529b\u3002", "method": "\u6211\u4eec\u6574\u7406\u3001\u6807\u51c6\u5316\u5e76\u5171\u4eab\u4e86\u5305\u542b9\u79cd\u6297\u539f\uff08\u6d89\u53ca\u6d41\u611f\u3001\u6297\u6eb6\u83cc\u9176\u3001HER2\u3001VEGF\u3001\u6574\u5408\u7d20\u548cSARS-CoV-2\uff09\u548c155,853\u4e2a\u91cd\u94fe\u7a81\u53d8\u6297\u4f53\u76849\u4e2a\u6570\u636e\u96c6\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e8614\u4e2a\u86cb\u767d\u8d28\u6a21\u578b\uff0c\u5305\u62ec\u63a9\u853d\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u3001\u9006\u6298\u53e0\u6a21\u578b\u3001\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u548c\u51e0\u4f55\u56fe\u6a21\u578b\u3002\u6a21\u578b\u53ef\u80fd\u6027\u4e0e\u5b9e\u9a8c\u4eb2\u548c\u529b\u503c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7ed3\u6784\u6761\u4ef6\u9006\u6298\u53e0\u6a21\u578b\u5728\u4eb2\u548c\u529b\u76f8\u5173\u6027\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u5728\u589e\u52a0\u6297\u4f53F045-092\u4e0e\u6297\u539f\u6d41\u611fH1N1\u7684\u7ed3\u5408\u4eb2\u548c\u529b\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u8868\u73b0\u6700\u4f73\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u65b9\u6cd5\u662f\u91c7\u6837\u4e00\u7ec4\u4e0e\u6297\u539f\u7ed3\u5408\u7684\u65b0\u6297\u4f53\uff0c\u5e76\u6839\u636eAb-Ag\u590d\u5408\u7269\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u751f\u7269\u7269\u7406\u7279\u6027\u5bf9\u5176\u8fdb\u884c\u6392\u5e8f\u3002", "conclusion": "AbBiBench\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u751f\u7269\u5b66\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u6709\u6548\u3001\u529f\u80fd\u611f\u77e5\u7684\u6297\u4f53\u8bbe\u8ba1\u6a21\u578b\u7684\u5f00\u53d1\u3002"}}
